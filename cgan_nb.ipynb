{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, im_chan: int = 1, hidden_dim: int = 64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            self._discriminator_block(im_chan, hidden_dim),\n",
    "            self._discriminator_block(hidden_dim, hidden_dim * 2),\n",
    "            self._discriminator_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _discriminator_block(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        output_channels: int,\n",
    "        kernel_size: int = 4,\n",
    "        stride: int = 2,\n",
    "        final_layer: int = False,\n",
    "    ):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(negative_slope=0.2),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.discriminator(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim: int = 10, im_chan: int = 1, hidden_dim: int = 64):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.generator = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 4),  # <- upsampling\n",
    "            self._generator_block(\n",
    "                hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1\n",
    "            ),\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim),\n",
    "            self._generator_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _generator_block(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        output_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 2,\n",
    "        final_layer: int = False,\n",
    "    ):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        # 2D -> 4D, i.e., adding H & W dimensions for ConvTranspose2d op\n",
    "        noise = noise.view(len(noise), self.input_dim, 1, 1)  # [B, C, H, W]\n",
    "        return self.generator(noise)\n",
    "\n",
    "\n",
    "def create_noise_vector(n_samples: int, input_dim: int, device: str = \"cuda\"):\n",
    "    return torch.randn(n_samples, input_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "MNIST_SHAPE = (1, 28, 28)\n",
    "N_CLASSES = 10\n",
    "N_EPOCHS = 200\n",
    "Z_DIM = 64\n",
    "DISPLAY_STEP = 100\n",
    "BATCH_SIZE = 128\n",
    "LR = 2e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CRITERION = nn.BCEWithLogitsLoss()\n",
    "TRANSFORMS = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]\n",
    ")\n",
    "DEBUG = False\n",
    "LOG_PATH = \"./logs\"\n",
    "BASE_DIR = \".\"\n",
    "CHECKPOINT_DIR = \"./checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Modules\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(0)  # Set for our testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def plot_images_from_tensor(\n",
    "    image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True\n",
    "):\n",
    "    image_tensor = (image_tensor + 1) / 2  # [-1, 1] -> [0, 1] (normalizes image)\n",
    "\n",
    "    # detach a tensor from the current computational graph and moving it to CPU.\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # [C, H, W] -> [H, W, C] (format expected by matplotlib)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "\n",
    "def ohe_vector_from_labels(label_tensor, n_classes):\n",
    "    # takes 'label_tensor' tensor of shape (*), and converts it to 0/1s tensor of shape (*, n_classes)\n",
    "    return F.one_hot(label_tensor, num_classes=n_classes)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.tensor([4, 3, 2, 1, 0])\n",
    "F.one_hot(x, num_classes=6)\n",
    "\n",
    "# Expected result\n",
    "# tensor([[0, 0, 0, 0, 1, 0],\n",
    "#         [0, 0, 0, 1, 0, 0],\n",
    "#         [0, 0, 1, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0],\n",
    "#         [1, 0, 0, 0, 0, 0]])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def concat_vectors(x, y):\n",
    "    # Generator in CGAN doesn't only take the noise vector 'z' but also the label vector 'y'.\n",
    "    # Hence, the CONCATENATION, i.e., Generator Input  - Noise + Label Vector\n",
    "    combined = torch.cat(tensors=(x.float(), y.float()), axis=1)\n",
    "    return combined\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Concatenation of Multiple Tensor with `torch.cat()`\n",
    "RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "    1. All tensors need to have the same number of dimensions, and\n",
    "    2. All dimensions EXCEPT the one that they are concatenated on, need to have the same size. \n",
    "\n",
    "Concatenation between (32,2,32) and (32,4,32) with concat(dim=1) yields (32,6,32). \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def calculate_input_dim(z_dim, mnist_shape, n_classes):\n",
    "    \"\"\"\n",
    "    DISCRIMINATOR -> Class information is appended as a channel or some other method,\n",
    "    GENERATOR -> Class information is encoded by appending a one-hot vector to the noise to form a long vector input.\n",
    "\n",
    "    z_dim = size of the noise vector - 64 or 128,\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10 [mnist digits]\n",
    "\n",
    "    \"\"\"\n",
    "    generator_input_dim = z_dim + n_classes  # latent noise [z] + label vector [y]\n",
    "\n",
    "    discriminator_image_channel = (\n",
    "        mnist_shape[0] + n_classes\n",
    "    )  # label information is appended as a channel.\n",
    "\n",
    "    return generator_input_dim, discriminator_image_channel\n",
    "\n",
    "\n",
    "def init_setting():\n",
    "    timestr = str(datetime.datetime.now().strftime(\"%Y-%m%d_%H%M\"))\n",
    "    experiment_dir = Path(LOG_PATH)\n",
    "    experiment_dir.mkdir(exist_ok=True)  # directory for saving experimental results\n",
    "    experiment_dir = experiment_dir.joinpath(timestr)\n",
    "    experiment_dir.mkdir(exist_ok=True)  # root directory of each experiment\n",
    "\n",
    "    checkpoint_dir = Path(CHECKPOINT_DIR)\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    checkpoint_dir = checkpoint_dir.joinpath(timestr)\n",
    "    checkpoint_dir.mkdir(exist_ok=True)  # root directory of each experiment\n",
    "\n",
    "    # returns several directory paths\n",
    "    return experiment_dir, checkpoint_dir, timestr\n",
    "\n",
    "\n",
    "def save_checkpoint(**kwargs):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m disc_fake_pred \u001b[38;5;241m=\u001b[39m disc(fake_image_and_labels)  \u001b[38;5;66;03m# [128, 1]\u001b[39;00m\n\u001b[1;32m     96\u001b[0m gen_loss \u001b[38;5;241m=\u001b[39m CRITERION(disc_fake_pred, torch\u001b[38;5;241m.\u001b[39mones_like(disc_fake_pred))\n\u001b[0;32m---> 97\u001b[0m \u001b[43mgen_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m gen_opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Keep track of average generator losses.\u001b[39;00m\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Miniconda/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exp_path, checkpoint_dir, timestr = init_setting()\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=MNIST(root=\"data\", download=False, transform=TRANSFORMS),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    gen_input_dim, disc_input_chan = calculate_input_dim(\n",
    "        z_dim=Z_DIM, mnist_shape=MNIST_SHAPE, n_classes=N_CLASSES\n",
    "    )  # (74, 11)\n",
    "\n",
    "    gen = Generator(input_dim=gen_input_dim).to(DEVICE)\n",
    "    gen_opt = torch.optim.Adam(params=gen.parameters(), lr=LR)\n",
    "    disc = Discriminator(im_chan=disc_input_chan).to(DEVICE)\n",
    "    disc_opt = torch.optim.Adam(params=disc.parameters(), lr=LR)\n",
    "\n",
    "    gen, disc = gen.apply(weights_init), disc.apply(weights_init)\n",
    "\n",
    "    generator_losses, discriminator_losses = [], []\n",
    "    cur_step = 0\n",
    "\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        for idx, (images, labels) in enumerate(dataloader):\n",
    "            cur_batch_size = len(images)  # 128\n",
    "            images = images.to(DEVICE)\n",
    "\n",
    "            \"\"\"\n",
    "            Create OHE vectors from labels (ground truth), i.e., \n",
    "            - labels[0] = 8\n",
    "            - one_hot_labels[0] = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0] \n",
    "            \"\"\"\n",
    "\n",
    "            one_hot_labels = ohe_vector_from_labels(\n",
    "                label_tensor=labels, n_classes=N_CLASSES\n",
    "            ).to(DEVICE)  # [128, 10]\n",
    "            image_one_hot_labels = one_hot_labels[..., None, None]  # [128, 10, 1, 1]\n",
    "\n",
    "            # [128, 10, 1, 1] -> # [128, 10, 28, 28]\n",
    "            image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "                1, 1, MNIST_SHAPE[1], MNIST_SHAPE[2]\n",
    "            )  # how many times to repeat each dim\n",
    "\n",
    "            ### Train Discriminator\n",
    "            disc_opt.zero_grad()\n",
    "            fake_noise = create_noise_vector(\n",
    "                n_samples=cur_batch_size, input_dim=Z_DIM, device=DEVICE\n",
    "            )  # [128, 64]\n",
    "\n",
    "            \"\"\"\n",
    "            IMPORTANT:\n",
    "            * For Generator, labels are appened to the end of the noise vectors.\n",
    "            * For Discriminator, labels are appended to the channel dimension.\n",
    "            \"\"\"\n",
    "\n",
    "            # z(noise) - [128, 64] + y(true_labels) - [128, 10]\n",
    "            noise_and_labels = concat_vectors(fake_noise, one_hot_labels)  # [128, 74]\n",
    "\n",
    "            # noise_and_labels dims get expanded automatically during generator's forward pass\n",
    "            fake = gen(noise_and_labels)  # CONDITIONED FAKE IMAGES / [128, 1, 28, 28]\n",
    "\n",
    "            # [128, 1, 28, 28] + [128, 10, 28, 28] = [128, 11, 28, 28] (both)\n",
    "            fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "            real_image_and_labels = concat_vectors(images, image_one_hot_labels)\n",
    "\n",
    "            # Getting the discriminator's predictions\n",
    "            disc_fake_pred = disc(fake_image_and_labels.detach())  # [128, 1]\n",
    "            disc_real_pred = disc(real_image_and_labels)  # [128, 1]\n",
    "\n",
    "            # Calculating the Loss\n",
    "            disc_fake_loss = CRITERION(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "            disc_real_loss = CRITERION(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "            disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "            # Backpropagate & Update Weights\n",
    "            disc_loss.backward(retain_graph=True)\n",
    "            disc_opt.step()\n",
    "\n",
    "            # Keep track of average discriminator losses.\n",
    "            discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "            ### Train Generator\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            # [128, 1, 28, 28] + [128, 10, 28, 28] -> [128, 11, 28, 28]\n",
    "            fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "            disc_fake_pred = disc(fake_image_and_labels)  # [128, 1]\n",
    "\n",
    "            gen_loss = CRITERION(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            # Keep track of average generator losses.\n",
    "            generator_losses += [gen_loss.item()]\n",
    "\n",
    "            if idx % DISPLAY_STEP == 0 and idx > 0:\n",
    "                # Calculate Generator Mean Loss for the latest display steps (i.e., last 50 steps)\n",
    "                gen_mean = sum(generator_losses[-DISPLAY_STEP:]) / DISPLAY_STEP\n",
    "                disc_mean = sum(discriminator_losses[-DISPLAY_STEP:]) / DISPLAY_STEP\n",
    "                print(\n",
    "                    f\"Epoch {epoch}: | Step: {idx} | Gen Loss: {gen_mean} | Disc Loss: {disc_mean}\"\n",
    "                )\n",
    "\n",
    "                plot_images_from_tensor(fake)\n",
    "                plot_images_from_tensor(images)\n",
    "            cur_step += 1\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"gen_state_dict\": gen.state_dict(),\n",
    "            \"disc_state_dict\": disc.state_dict(),\n",
    "            \"gen_optimizer\": gen_opt.state_dict(),\n",
    "            \"disc_optimizer\": disc_opt.state_dict(),\n",
    "        }  # save state dictionary\n",
    "        torch.save(checkpoint, f\"{checkpoint_dir}/model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
